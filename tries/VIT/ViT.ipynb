{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Core\n",
    "import os, time, math, random\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Metrics & plots\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "# Use the returned path to set base_dir correctly\n",
    "base_dir = os.path.join(path, \"chest_xray\")\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir   = os.path.join(base_dir, 'val')\n",
    "test_dir  = os.path.join(base_dir, 'test')"
   ],
   "id": "337e755a13b3f6d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Train dir:\", train_dir)\n",
    "print(\"Val dir:\", val_dir)\n",
    "print(\"Test dir:\", test_dir)\n",
    "\n",
    "print(\"Train dir contents:\", os.listdir(train_dir))\n",
    "print(\"Val dir contents:\", os.listdir(val_dir))\n",
    "print(\"Test dir contents:\", os.listdir(test_dir))"
   ],
   "id": "217e311e844fe367"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "NUM_WORKERS = 2  # set higher if your env allows\n",
    "PATIENCE = 5      # early stopping"
   ],
   "id": "35f42519666a6030"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transforms & dataloaders\n",
    "\n",
    "# Note: X-ray images are stored as RGB in this dataset. We normalize like ImageNet for stability.\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(7),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=transform)\n",
    "test_ds  = datasets.ImageFolder(test_dir,  transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "class_names"
   ],
   "id": "66d282ce992033df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Handle class imbalance (loss weights)\n",
    "\n",
    "# Compute class weights from train set for BCEWithLogitsLoss (binary labels 0/1)\n",
    "# ImageFolder encodes classes alphabetically; ensure 'NORMAL' and 'PNEUMONIA' mapping if needed.\n",
    "from collections import Counter\n",
    "train_counts = Counter([y for _, y in train_ds.samples])\n",
    "print(\"Train counts:\", train_counts, \" -> class_names:\", class_names)\n",
    "\n",
    "# weight for positive class (assume class 1 is 'PNEUMONIA' alphabetically after 'NORMAL')\n",
    "pos_weight_value = train_counts[0] / max(1, train_counts[1])  # ratio negatives/positives\n",
    "pos_weight = torch.tensor([pos_weight_value], device=device, dtype=torch.float32)\n",
    "pos_weight"
   ],
   "id": "fd7d05818158fe21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#  Handle class imbalance (loss weights)\n",
    "\n",
    "# Compute class weights from train set for BCEWithLogitsLoss (binary labels 0/1)\n",
    "# ImageFolder encodes classes alphabetically; ensure 'NORMAL' and 'PNEUMONIA' mapping if needed.\n",
    "from collections import Counter\n",
    "train_counts = Counter([y for _, y in train_ds.samples])\n",
    "print(\"Train counts:\", train_counts, \" -> class_names:\", class_names)\n",
    "\n",
    "# weight for positive class (assume class 1 is 'PNEUMONIA' alphabetically after 'NORMAL')\n",
    "pos_weight_value = train_counts[0] / max(1, train_counts[1])  # ratio negatives/positives\n",
    "pos_weight = torch.tensor([pos_weight_value], device=device, dtype=torch.float32)\n",
    "pos_weight"
   ],
   "id": "15cf80815ae0a4ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ViT building blocks: Patch Embedding\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_ch=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.grid_size = img_size // patch_size\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C, H, W]\n",
    "        x = self.proj(x)                 # [B, E, H', W']\n",
    "        x = x.flatten(2).transpose(1, 2) # [B, N, E]\n",
    "        return x"
   ],
   "id": "ba07e9039da9fcbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Multi-head self-attention (from scratch)\n",
    "\n",
    "class MSA(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_heads=6, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):  # x: [B, N, E]\n",
    "        B, N, E = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)  # each: [B, N, H, D]\n",
    "        q = q.transpose(1, 2)        # [B, H, N, D]\n",
    "        k = k.transpose(1, 2)        # [B, H, N, D]\n",
    "        v = v.transpose(1, 2)        # [B, H, N, D]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B,H,N,N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = attn @ v                                # [B,H,N,D]\n",
    "        out = out.transpose(1, 2).reshape(B, N, E)    # [B,N,E]\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ],
   "id": "c02ad52a027571b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transformer encoder block (Pre-LN)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=384, num_heads=6, mlp_ratio=4.0, drop=0.1, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = MSA(embed_dim, num_heads, attn_drop, proj_drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        hidden = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ],
   "id": "6b9a59a0ee7df7ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Vision Transformer model\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_ch=3, num_classes=1,\n",
    "                 embed_dim=384, depth=8, num_heads=6, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_ch, embed_dim)\n",
    "        self.num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # class token & positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop=drop, attn_drop=0.0)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)  # logits\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.head.weight, std=0.02)\n",
    "        if self.head.bias is not None:\n",
    "            nn.init.zeros_(self.head.bias)\n",
    "\n",
    "        # optional: kaiming for linear/conv\n",
    "        def _init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "        self.apply(_init)\n",
    "\n",
    "    def forward(self, x):  # x: [B,3,224,224]\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # [B, N, E]\n",
    "        cls = self.cls_token.expand(B, -1, -1)  # [B,1,E]\n",
    "        x = torch.cat([cls, x], dim=1)          # [B, 1+N, E]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]                       # [B, E]\n",
    "        logits = self.head(cls_out)             # [B, 1]\n",
    "        return logits"
   ],
   "id": "e7effa52efee37f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Instantiate model, optimizer, scheduler, loss\n",
    "\n",
    "model = ViT(\n",
    "    img_size=IMG_SIZE, patch_size=16, in_ch=3, num_classes=1,\n",
    "    embed_dim=384, depth=8, num_heads=6, mlp_ratio=4.0, drop=0.1\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())/1e6\n",
    "print(model.__class__.__name__, \"params (M):\", round(total_params, 2))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # handles imbalance\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))"
   ],
   "id": "642dd8b74f524bf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Utilities: metrics function\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_from_logits(logits, targets):\n",
    "    # logits: [N,1], targets: [N]\n",
    "    probs = torch.sigmoid(logits.view(-1)).cpu().numpy()\n",
    "    preds = (probs >= 0.5).astype(np.int64)\n",
    "    y_true = targets.view(-1).cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except ValueError:\n",
    "        auc = float('nan')  # only one class in batch\n",
    "    return acc, precision, recall, f1, auc, preds, probs\n"
   ],
   "id": "6c78b2e4c1ab0f33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train/validate loops (1 epoch helper)\n",
    "\n",
    "def run_one_epoch(loader, train_mode=True):\n",
    "    model.train(train_mode)\n",
    "    epoch_loss = 0.0\n",
    "    all_logits, all_targets = [], []\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True).float()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
    "            logits = model(images).view(-1)  # [B]\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "        all_logits.append(logits.detach().unsqueeze(1))\n",
    "        all_targets.append(targets.detach())\n",
    "\n",
    "    epoch_loss /= len(loader.dataset)\n",
    "    all_logits = torch.cat(all_logits, dim=0)   # [N,1]\n",
    "    all_targets = torch.cat(all_targets, dim=0) # [N]\n",
    "    acc, prec, rec, f1, auc, preds, probs = compute_metrics_from_logits(all_logits, all_targets)\n",
    "    return epoch_loss, acc, prec, rec, f1, auc"
   ],
   "id": "56f88cf149c3bbb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[],\n",
    "           \"train_f1\":[], \"val_f1\":[], \"train_auc\":[], \"val_auc\":[]}\n",
    "\n",
    "best_val = -np.inf\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc, tr_prec, tr_rec, tr_f1, tr_auc = run_one_epoch(train_loader, train_mode=True)\n",
    "    va_loss, va_acc, va_prec, va_rec, va_f1, va_auc = run_one_epoch(val_loader,   train_mode=False)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(va_loss)\n",
    "    history[\"train_acc\"].append(tr_acc);   history[\"val_acc\"].append(va_acc)\n",
    "    history[\"train_f1\"].append(tr_f1);     history[\"val_f1\"].append(va_f1)\n",
    "    history[\"train_auc\"].append(tr_auc);   history[\"val_auc\"].append(va_auc)\n",
    "\n",
    "    # monitor val AUC primarily (ViT benefits from good calibration); fallback to F1 if NaN\n",
    "    score = va_auc if not np.isnan(va_auc) else va_f1\n",
    "    if score > best_val:\n",
    "        best_val = score\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"tr_loss {tr_loss:.4f} acc {tr_acc:.3f} f1 {tr_f1:.3f} auc {tr_auc:.3f} || \"\n",
    "          f\"val_loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f} auc {va_auc:.3f} \"\n",
    "          f\"[{time.time()-t0:.1f}s]\")\n",
    "\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total time: {(time.time()-start_time)/60:.1f} min\")\n",
    "# load best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ],
   "id": "9cbfef308116f241"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_curves(history):\n",
    "    epochs = range(1, len(history[\"train_loss\"])+1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_loss\"],   label=\"val\")\n",
    "    plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_acc\"],   label=\"val\")\n",
    "    plt.title(\"Accuracy\"); plt.xlabel(\"epoch\"); plt.legend()\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(epochs, history[\"train_f1\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_f1\"],   label=\"val\")\n",
    "    plt.title(\"F1-score\"); plt.xlabel(\"epoch\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_curves(history)\n"
   ],
   "id": "5c0c218887e5d93a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_logits, all_targets = [], []\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device); targets = targets.to(device)\n",
    "        logits = model(images).detach().cpu()\n",
    "        all_logits.append(logits)\n",
    "        all_targets.append(targets.cpu())\n",
    "    all_logits = torch.cat(all_logits, dim=0)   # [N,1]\n",
    "    all_targets = torch.cat(all_targets, dim=0) # [N]\n",
    "\n",
    "    acc, prec, rec, f1, auc, preds, probs = compute_metrics_from_logits(all_logits, all_targets)\n",
    "    cm = confusion_matrix(all_targets.numpy(), preds)\n",
    "    return {\"acc\":acc,\"precision\":prec,\"recall\":rec,\"f1\":f1,\"auc\":auc,\"cm\":cm,\"probs\":probs,\"preds\":preds}\n",
    "\n",
    "test_metrics = evaluate(test_loader)\n",
    "test_metrics\n"
   ],
   "id": "787a27a3e76f950b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"=== Test set performance ===\")\n",
    "print(f\"Accuracy : {test_metrics['acc']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall   : {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score : {test_metrics['f1']:.4f}\")\n",
    "print(f\"AUC      : {test_metrics['auc']:.4f}\")\n",
    "print(\"Confusion matrix (rows: true [0,1], cols: pred [0,1]):\\n\", test_metrics[\"cm\"])\n"
   ],
   "id": "902edd6df458ef95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
