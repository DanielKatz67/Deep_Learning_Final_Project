{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T10:25:03.879118Z",
     "start_time": "2025-08-09T10:25:03.874005Z"
    }
   },
   "source": [
    "# Setup (PyTorch, paths, device, seeds)\n",
    "import os, random, math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "torch.manual_seed(42)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1084b15d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T10:25:07.888134Z",
     "start_time": "2025-08-09T10:25:07.131430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "# Use the returned path to set base_dir correctly\n",
    "base_dir = os.path.join(path, \"chest_xray\")\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir   = os.path.join(base_dir, 'val')\n",
    "test_dir  = os.path.join(base_dir, 'test')"
   ],
   "id": "bc7a8df29b6d692a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T10:25:10.838488Z",
     "start_time": "2025-08-09T10:25:10.834899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train dir:\", train_dir)\n",
    "print(\"Val dir:\", val_dir)\n",
    "print(\"Test dir:\", test_dir)\n",
    "\n",
    "print(\"Train dir contents:\", os.listdir(train_dir))\n",
    "print(\"Val dir contents:\", os.listdir(val_dir))\n",
    "print(\"Test dir contents:\", os.listdir(test_dir))"
   ],
   "id": "ef61bd67727bf5c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir: /Users/daniel/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray/train\n",
      "Val dir: /Users/daniel/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray/val\n",
      "Test dir: /Users/daniel/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2/chest_xray/test\n",
      "Train dir contents: ['.DS_Store', 'PNEUMONIA', 'NORMAL']\n",
      "Val dir contents: ['PNEUMONIA', 'NORMAL']\n",
      "Test dir contents: ['PNEUMONIA', 'NORMAL']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T10:36:23.240851Z",
     "start_time": "2025-08-09T10:36:23.198509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Datasets, DataLoaders, class weights (for imbalance)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomRotation(10),\n",
    "    # transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    # normalize to [-1,1] range (since we train from scratch)\n",
    "    # transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])\n",
    "\n",
    "# eval_tfms = transforms.Compose([\n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "# ])\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=transform)\n",
    "val_ds   = datasets.ImageFolder(val_dir,   transform=transform)\n",
    "test_ds  = datasets.ImageFolder(test_dir,  transform=transform)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes, \"=>\", train_ds.class_to_idx)"
   ],
   "id": "4b402c8e3784eced",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['NORMAL', 'PNEUMONIA'] => {'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CNN model as a class (__init__ + forward)\n",
    "class PneumoCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(PneumoCNN, self).__init__()\n",
    "        # feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 112x112\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 56x56\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 28x28\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 14x14\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),   # 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256*7*7, 512), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        ## Do we need to reshape ?\n",
    "        x = self.classifier(x)\n",
    "        return x  # logits if num_classes=1; shape [B,1]"
   ],
   "id": "b64e01101c80d2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = PneumoCNN(num_classes=1)\n",
    "model"
   ],
   "id": "2cbc1b2e66ef69d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T10:41:14.294111Z",
     "start_time": "2025-08-09T10:41:14.291365Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 17,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "model = model.to(device)\n",
    "EPOCHS = 15\n",
    "patience = 4\n",
    "lr = 1e-3\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ],
   "id": "557e63e888f13842"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T10:41:16.079738Z",
     "start_time": "2025-08-09T10:41:16.068763Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts -> NEG(NORMAL)=1341, POS(PNEUMONIA)=3875, pos_weight=0.346\n"
     ]
    }
   ],
   "execution_count": 18,
   "source": [
    "# Compute class weights for BCEWithLogitsLoss (pos_weight = N_neg / N_pos for label=1 class)\n",
    "# In ImageFolder, NORMAL -> 0, PNEUMONIA -> 1 (alphabetical)\n",
    "train_targets = torch.tensor([y for _, y in train_ds.samples])\n",
    "pos = (train_targets==1).sum().item()\n",
    "neg = (train_targets==0).sum().item()\n",
    "pos_weight = torch.tensor([neg / max(pos,1.0)], dtype=torch.float32)\n",
    "print(f\"Train counts -> NEG(NORMAL)={neg}, POS(PNEUMONIA)={pos}, pos_weight={pos_weight.item():.3f}\")"
   ],
   "id": "6bb13bf190bf4458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loss, Optimizer, LR scheduler\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))  # robust for class imbalance\n",
    "## TODO: consider changing to nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True) # monitors val accuracy"
   ],
   "id": "d38cc78f1fc3d7ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Utilities: train/eval loops\n",
    "def accuracy_from_logits(logits, targets):\n",
    "    # logits: [B,1], targets: [B]\n",
    "    probs = torch.sigmoid(logits).squeeze(1)\n",
    "    preds = (probs >= 0.5).long()\n",
    "    correct = (preds == targets.long()).sum().item()\n",
    "    return correct, preds, probs\n",
    "\n",
    "def run_one_epoch(model, loader, epochNumber, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    n = 0\n",
    "    all_probs, all_targets = [], []\n",
    "\n",
    "    for images, targets in tqdm(loader, desc=f\"Epoch {epochNumber}/{EPOCHS}\"):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(images).squeeze(1)\n",
    "            loss = criterion(logits, targets.float())\n",
    "            if is_train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        c, _, probs = accuracy_from_logits(logits.unsqueeze(1), targets)\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "        correct += c\n",
    "        n += images.size(0)\n",
    "\n",
    "        all_probs.append(probs.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "\n",
    "    avg_loss = epoch_loss / n\n",
    "    acc = correct / n\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # Some environments may lack enough positive/negative samples for AUC early on; guard it\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc, all_probs, all_targets"
   ],
   "id": "ce630f22a6e4cb5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CTraining over epochs with early stopping + checkpoint\n",
    "best_val_acc = -np.inf\n",
    "no_improve = 0\n",
    "ckpt_path = \"best_cnn_pneumonia.pt\"\n",
    "\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"train_auc\":[],\n",
    "           \"val_loss\":[], \"val_acc\":[], \"val_auc\":[]}\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_auc, _, _ = run_one_epoch(model, train_loader, epoch, optimizer)\n",
    "    va_loss, va_acc, va_auc, _, _ = run_one_epoch(model, val_loader, epoch, optimizer=None)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc); history[\"train_auc\"].append(tr_auc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc);   history[\"val_auc\"].append(va_auc)\n",
    "\n",
    "    # scheduler.step(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"Train: loss {tr_loss:.4f} acc {tr_acc:.4f} auc {tr_auc:.4f} | \"\n",
    "          f\"Val:   loss {va_loss:.4f} acc {va_acc:.4f} auc {va_auc:.4f}\")\n",
    "\n",
    "    # Early stopping & checkpoint\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        no_improve = 0\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"epoch\": epoch}, ckpt_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "print(f\"Training finished in {(time.time()-start)/60:.1f} min. Best val acc: {best_val_acc:.4f}\")"
   ],
   "id": "6940f5dd7c9308fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best checkpoint (optional) and plot curves\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "def plot_curves(h):\n",
    "    fig, axs = plt.subplots(1,3, figsize=(16,4))\n",
    "    axs[0].plot(h[\"train_loss\"], label=\"train\"); axs[0].plot(h[\"val_loss\"], label=\"val\")\n",
    "    axs[0].set_title(\"Loss\"); axs[0].legend()\n",
    "    axs[1].plot(h[\"train_acc\"], label=\"train\"); axs[1].plot(h[\"val_acc\"], label=\"val\")\n",
    "    axs[1].set_title(\"Accuracy\"); axs[1].legend()\n",
    "    axs[2].plot(h[\"train_auc\"], label=\"train\"); axs[2].plot(h[\"val_auc\"], label=\"val\")\n",
    "    axs[2].set_title(\"ROC-AUC\"); axs[2].legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_curves(history)"
   ],
   "id": "b6edbe44d7058147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final test evaluation + confusion matrix, precision/recall/F1\n",
    "test_loss, test_acc, test_auc, probs, targets = run_one_epoch(model, test_loader, 1, optimizer=None)\n",
    "print(f\"test pres (first 10): {probs[:10]}\")\n",
    "## TODO: consider using a threshold other than 0.5\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(f\"TEST â€” loss: {test_loss:.4f} | acc: {test_acc:.4f} | auc: {test_auc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(targets, preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(targets, preds, target_names=test_ds.classes, digits=4))\n",
    "\n",
    "# Quick ROC curve\n",
    "fpr, tpr, _ = roc_curve(targets, probs)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(fpr, tpr, lw=2)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve (Test)')\n",
    "plt.grid(True); plt.show()"
   ],
   "id": "2843d787f59a7656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inference helper (predict on a few samples)\n",
    "from PIL import Image\n",
    "\n",
    "def predict_image(path, model, tfms=transform):\n",
    "    model.eval()\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    x = tfms(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logit = model(x)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "    pred = int(prob >= 0.5)\n",
    "    return prob, pred  # probability of PNEUMONIA, predicted label (0 normal / 1 pneumonia)\n",
    "\n",
    "# Example:\n",
    "# p, yhat = predict_image(os.path.join(test_dir, \"PNEUMONIA\", os.listdir(os.path.join(test_dir,\"PNEUMONIA\"))[0]), model)\n",
    "# print(p, yhat)\n"
   ],
   "id": "91f49abba928a490"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: consider adding Visualizations like the R3",
   "id": "a4a8d86f0508e0a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
